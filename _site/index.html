<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <link rel="stylesheet" href="main.css">
  <link rel="icon" type="image/ico" href="favicon.ico">

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-112406581-4', 'auto');
  ga('send', 'pageview');
  </script>
  <!-- End Google Analytics -->

  <title>Amlaan Bhoi</title>
</head>
<body style="background-color: #f9f9f9">
 <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
  <tr>
    <td>
      <table>
        <tr>
          <td width="75%" valign="middle">
            <name>Amlaan Bhoi</name> <foot style="font-size: 0.70em">(um-LAAN bho-EE)</foot>
            <br>
            Seattle, WA
            <p></p>
            I am an applied scientist at <a href="http://www.amazon.com/">Amazon</a> in the WW Returns, ReCommerce & Sustainability organization working on computer vision and natural language processing systems in the Reverse Logistics domain. <br><br> I graduated with a Master in Computer Science from <a href="https://www.uic.edu/">University of Illinois at Chicago</a> with a focus on machine learning & computer vision where I was fortunate enough to be advised by <a href="https://www.cs.uic.edu/~zhangx/">Xinhua Zhang</a>. I was also an <a href="https://software.intel.com/en-us/ai-academy/ambassadors">Intel AI Ambassador</a> where I shared upcoming research on ML & computer vision. Before that, graduated from <a href="http://www.amity.edu/">Amity University</a> with a Bachelor of Technology in Computer Science & Engineering in May 2017 with First Class honors where I was advised by <a href="https://www.nitw.ac.in/faculty/id/16877/">Sushil Kumar</a>.
            <p align="center">
              <a href="mailto:amlaanb@gmail.com">E-mail</a> &nbsp|&nbsp
              <a href="cv/cv.pdf"><emphasis>Curriculum Vitae</emphasis></a> &nbsp|&nbsp
              <a href="https://scholar.google.com/citations?user=6zkTbN4AAAAJ&hl=en">Publications</a>
              &nbsp|&nbsp
              <a href="http://www.linkedin.com/in/abhoi/"> LinkedIn </a> &nbsp|&nbsp
              <a href="https://github.com/abhoi"> Github </a> &nbsp|&nbsp
              <a href="https://www.twitter.com/amlaanb"> Twitter </a>
              <!--<span style="background-color: #FFFF00"><a href="https://abhoi.github.io/blog/"><b> Blog<span>&#42;</span> </b></a></span>-->
          </p>
            </td>
            <td width="40%">
              <a href="#top"><img class="profile-pic" src="images/profile_pic.jpg"></a>
        <!-- <center>
        <img src="images/logos.png" width=25%>
      </center> -->
    </td>
  </tr>
</table>

<br>

<table>
  <tr>
    <td width="100%" valign="middle">
      <heading>Research Interests</heading>
      <p>
      My research interests lie in computer vision, image processing, machine learning, optimization, and statistical learning theory. I'm also passionate about augmented reality & computational photography. I also dabble around with topics on self-supervised learning, distributed machine learning, and monocular depth estimation.
      <br><br>
      My thesis titled <u>Invariant Kernels for Few-shot Learning</u> generalized the idea of orbit embeddings (projection of embeddings to low-dimensional feature space) for invariance modeling to patch-based image classification in the few-shot learning setting. In simple terms, it guaranteed strong invariance to data augmentations/perturbations to images such as <i>rotation</i>, <i>translation</i>, <i>etc.</i> in few-shot learning.
  </p>
    </td>
  </tr>
</table>

<br>

<table>
  <tr>
    <td>
      <heading>News</heading>
      <ul>
      	<li>Joined Amazon as an Applied Scientist in the WW Returns, Recommerce & Sustainability organization.</li>
      	<li>Awarded <a href="https://grad.uic.edu/news-stories/fall19-award-winners/"><b>Outstanding Thesis Award</b></a> at UIC during Fall 2019. Only Master's Thesis in a cohort of five theses awarded this title.</li>
      	<li>Joined CCC Information Services as a Sr. Data Scientist, Computer Vision.</li>
      	<li>Continuing internship at CCC Information Services for the Fall 2018, Winter 2018, and Spring 2019 semester.</li>
      	<li>Featured on Intel's website discussing my work, computer vision problems, and how Intel can help. You can check it out <a href="https://software.intel.com/en-us/blogs/2018/08/03/ai-student-ambassador-amlaan-bhoi-solving-computer-vision-problems-through-machine">here</a>.</li>
      	<li>Presented poster on <a href="https://www.dropbox.com/s/4329409ls1t5m2i/DenseNet-poster.pdf?dl=0">Tiramisu DenseNet Architecture for Precise Segmentation</a> at Intel AI Booth at <a href="https://www.linkedin.com/feed/update/urn:li:activity:6415319016641949696/">CVPR 2018</a>.</li>
      	<li>Joined as an R&D Intern, Computer Vision in the Photo Analytics and Machine Learning group at CCC Information Services.</li>
      	<li>Joined Intel AI Ambassador Program.</li>
      	<li>Mentioned in <a href="https://news.ucsc.edu/2018/01/cruzhacks.html">UCSC newsletter</a> for developing a low-poly VR application (Google Pixel 2 + DayDream) at CruzHacks 2017.</li>
      	<li>Awarded <a href="https://devpost.com/software/lifeguard-io">Best Microsoft Hack</a> at HackHarvard 2017.</li>
      	<li>Awarded Best Technical Innovation at Amity University Convocation 2017.</li>
      </ul>
    </td>
  </tr>
</table>

<!-- To fix heading alignment -->
<table>
  <tr>
    <td><heading>Papers</heading></td>
  </tr>
</table>

<table cellspacing="10">
  <tr onmouseout="depth_stop()" onmouseover="depth_start()">
    <td width="25%">
      <div class="one">
        <div class="two" id = 'depth_image'><img class="portrait_image" src='papers/depth/after.png'></div>
        <img class="portrait_image" src='papers/depth/before.png'>
      </div>
      <script type="text/javascript">
        function depth_start() {
          document.getElementById('depth_image').style.opacity = "1";
        }
        function depth_stop() {
          document.getElementById('depth_image').style.opacity = "0";
        }
        depth_stop()
      </script>
    </td>
    <td valign="top" width="75%">
      <a href="https://arxiv.org/abs/1901.09402">
        <papertitle>Monocular Depth Estimation: A Survey</papertitle>
      </a>
      <br>
      <strong>Amlaan Bhoi</strong>
      <br>
      <em>arXiv Preprint</em>, 2019 <br>
      <a href="https://arxiv.org/abs/1901.09402">arxiv</a>
      <br>
      Monocular depth estimation is often described as an ill-posed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3D object recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.
    </td>
  </tr>
  <tr onmouseout="star_stop()" onmouseover="star_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'star_image'><img class="portrait_image" src='papers/star/after.png'></div>
        <img class="portrait_image" src='papers/star/before.png'>
      </div>
      <script type="text/javascript">
        function star_start() {
          document.getElementById('star_image').style.opacity = "1";
        }
        function star_stop() {
          document.getElementById('star_image').style.opacity = "0";
        }
        star_stop()
      </script>
    </td>
    <td valign="top" width="75%">
      <a href="https://arxiv.org/pdf/1901.09403">
        <papertitle>Spatio-temporal Action Recognition: A Survey</papertitle>
      </a>
      <br>
      <strong>Amlaan Bhoi</strong>
      <br>
      <em>arXiv Preprint</em>, 2019 <br>
      <a href="https://arxiv.org/pdf/1901.09403">arxiv</a>
      <br>
      The task of action recognition or action detection involves analyzing videos and determining what action or motion is being performed. The primary subject of these videos are predominantly humans performing some action. However, this requirement can be relaxed to generalize over other subjects such as animals or robots. The applications can range from anywhere between human-computer inter-action to automated video editing proposals. When we consider spatiotemporal action recognition, we deal with action localization. This task not only involves determining what action is being performed but also when and where it is being performed in said video. This paper aims to survey the plethora of approaches and algorithms attempted to solve this task, give a comprehensive comparison between them, explore various datasets available for the problem, and determine the most promising approaches.
    </td>
  </tr>
  <tr onmouseout="style_stop()" onmouseover="style_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'style_image'><img class="portrait_image" src='papers/style/after.png'></div>
        <img class="portrait_image" src='papers/style/before.png'>
      </div>
      <script type="text/javascript">
        function style_start() {
          document.getElementById('style_image').style.opacity = "1";
        }
        function style_stop() {
          document.getElementById('style_image').style.opacity = "0";
        }
        style_stop()
      </script>
    </td>
    <td valign="top" width="75%">
     <a href="https://arxiv.org/abs/1806.00868">
      <papertitle>A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer</papertitle>
    </a>
    <br>
    <a href="https://github.com/titu1994">Somshubra Majumdar</a>,
    <strong>Amlaan Bhoi</strong>,
    <a href="https://www.linkedin.com/in/ganeshjcs">Ganesh Jagadeesan</a>
    <br>
    <em>arXiv Preprint</em>, 2018 <br>
    <a href="https://arxiv.org/abs/1806.00868">arxiv</a>
    |
    <a href="https://github.com/titu1994/Neural-Style-Transfer">code</a>
    <br>
    Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: <i>neural style transfer with improvements</i> and <i>universal style transfer</i>. We also make a comparison between the different images produced and how they can be qualitatively measured.
  </td>
</tr>
<tr>
  <td width="25%"><img src="papers/absa/before.png" alt="3DSP" width="160" height="120" style="border-style: none">
    <td valign="top" width="75%">
     <a href="https://arxiv.org/abs/1805.01984">
      <papertitle>Various Approaches to Aspect-based Sentiment Analysis</papertitle>
    </a>
    <br>
    <strong>Amlaan Bhoi</strong>,
    <a href="https://sandeepjoshi1910.github.io/">Sandeep Joshi</a>
    <br>
    <em>arXiv Preprint</em>, 2018 <br>
    <a href="https://arxiv.org/abs/1805.01984">arxiv</a>
    |
    <a href="https://github.com/abhoi/memnet-absa">code</a>
    <br>
    The problem of aspect-based sentiment analysis deals with classifying sentiments (negative, neutral, positive) for a given aspect in a sentence. A traditional sentiment classification task involves treating the entire sentence as a text document and classifying sentiments based on all the words. Let us assume, we have a sentence such as "the <i>acceleration</i> of this car is fast, but the <i>reliability</i> is horrible". This can be a difficult sentence because it has two aspects with conflicting sentiments about the same entity. Considering machine learning techniques (or deep learning), how do we encode the information that we are interested in one aspect and its sentiment but not the other? Let us explore various pre-processing steps, features, and methods used to facilitate in solving this task.
  </td>
</tr>
</table>

<br>

<!-- To fix heading alignment -->
<table>
  <tr>
    <td><heading>Projects</heading></td>
  </tr>
</table>

<table cellspacing="10">
  <tr onmouseout="iris_stop()" onmouseover="iris_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'iris_image'><img class="portrait_image" src='projects/iris/after.gif'></div>
        <img class="portrait_image" src='projects/iris/after.gif'>
      </div>
      <script type="text/javascript">
        function iris_start() {
          document.getElementById('iris_image').style.opacity = "1";
        }
        function iris_stop() {
          document.getElementById('iris_image').style.opacity = "0";
        }
        iris_stop()
      </script>
    </td>
    <td valign="top" width="75%">
      <a href="https://github.com/titu1994/Advanced_Machine_Learning">
        <papertitle>Iris: Speech to Code Converter</papertitle>
      </a>
      <br>
      <strong>Amlaan Bhoi</strong>,
      <a href="https://www.linkedin.com/in/shubadra-govindan/">Shubadra Govindan</a>,
      <a href="https://sandeepjoshi1910.github.io/">Sandeep Joshi</a>,
      <a href="https://dkaushik94.github.io">Debojit Kaushik</a>
      <br>
      <em>HackHarvard, 2018</em><br>
      <a href="https://devpost.com/software/iris-1f36ns">devpost</a>
      |
      <a href="https://github.com/abhoi/Speech2Code/">code</a>
      <br>
      A semantic speech to code generator.
      <ul>
        <li>Trained an intent classification model in Microsoft LUIS to recognize 15+ commands.</li>
        <li>Implemented a message passing protocol using RabbitMQ to talk between backend scripts, ElectronJS, and Visual Code extension.</li>
        <li>Wrote Python API wrappers for Microsoft LUIS and Google Cloud Speech API.</li>
      </ul>
    </td>
  </tr>

  <tr onmouseout="ocr_stop()" onmouseover="ocr_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'ocr_image'><img class="portrait_image" src='projects/ocr/after.png'></div>
        <img class="portrait_image" src='projects/ocr/before.png'>
      </div>
      <script type="text/javascript">
        function ocr_start() {
          document.getElementById('ocr_image').style.opacity = "1";
        }
        function ocr_stop() {
          document.getElementById('ocr_image').style.opacity = "0";
        }
        ocr_stop()
      </script>
    </td>
    <td valign="top" width="75%">
     <a href="https://github.com/titu1994/Advanced_Machine_Learning">
      <papertitle>Conditional Random Fields for Structured Output Prediction</papertitle>
    </a>
    <br>
    <strong>Amlaan Bhoi</strong>,
    <a href="https://github.com/titu1994">Somshubra Majumdar</a>,
    <a href="https://www.linkedin.com/in/ganeshjcs">Ganesh Jagadeesan</a>
    <br>
    <em>Advanced Machine Learning</em>, Spring 2018 <br>
    <a href="https://github.com/titu1994/Advanced_Machine_Learning">code</a>
    |
    <a href="https://github.com/titu1994/Advanced_Machine_Learning/blob/master/Project2-PETSc/Latex/Project_2.pdf">report</a>
    <br>
    An optical character recognition system to detect letters and words using conditional random fields.
    <ul>
     <li>Implemented linear-chain Conditional Random Fields from scratch to detect characters on UPenn OCR dataset.</li>
     <li>Implemented the Viterbi algorithm for forward-backward message passing between nodes, calculated the log probabilities and gradients, and used LBFGS solver to reach convergence.</li>
     <li>Achieved 84% letter-wise accuracy with dynamic programming implementation.</li>
     <li>Wrote a PETSc/Tao version to run on <a href="https://acer.uic.edu/">ACER</a> cluster in parallel using MPI code.</li>
     <li>Implemented SGD with Nestorov Momentum, AMSGrad, and Adam with MCMC for CRFs to compare with LBFGS implementation and plot comparison charts on different &#955 values.</li>
   </ul>
 </td>
</tr>

<tr onmouseout="friendly_stop()" onmouseover="friendly_start()" >
  <td width="25%">
    <div class="one">
      <div class="two" id = 'friendly_image'><img class="portrait_image" src='projects/apriori/after.png'></div>
      <img class="portrait_image" src='projects/apriori/before.png'>
    </div>
    <script type="text/javascript">
      function friendly_start() {
        document.getElementById('friendly_image').style.opacity = "1";
      }
      function friendly_stop() {
        document.getElementById('friendly_image').style.opacity = "0";
      }
      friendly_stop()
    </script>
  </td>
  <td valign="top" width="75%">
    <a href="https://github.com/abhoi/ms-apriori">
      <papertitle>MS Apriori: Rule Mining with Multiple Minimum Supports</papertitle></a><br>
      <strong>Amlaan Bhoi</strong>, <a href="https://sandeepjoshi1910.github.io/">Sandeep Joshi</a>
      <br>
      <em>Data Mining & Text Mining</em>, Spring 2018 <br>
      <a href="https://github.com/abhoi/ms-apriori">code</a>
      <br>
      An association rule mining (unsupervised learning) algorithm with multiple minimum support. This algorithm can be used for product recommendations based on historical data.
    </td>
  </tr>

  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'aperture_image'><img class="portrait_image" src='projects/alethea/after.png'></div>
        <img class="portrait_image" src='projects/alethea/after.png'>
      </div>
      <script type="text/javascript">
        function aperture_start() {
          document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
          document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
      </script>
    </td>
    <td valign="top" width="75%">
     <a href="https://dkaushik94.github.io/vis-alethea/">
      <papertitle>Alethea: Data science, visualization, and analysis</papertitle>
    </a>
    <br>
    <a href="https://github.com/titu1994">Somshubra Majumdar</a>,
    <strong>Amlaan Bhoi</strong>,
    <a href="https://github.com/dkaushik94">Debojit Kaushik</a>,
    <a href="https://github.com/calphones">Christopher Alphones</a>
    <br>
    <em>Introduction to Data Science</em>, Spring 2018 <br>
    <a href="https://github.com/dkaushik94/vis-alethea">code</a>
    |
    <a href="https://dkaushik94.github.io/vis-alethea/">demo</a>
    <br>
    An ETL pipeline, visualization, classical ML prediction, and ML & DL sentiment analysis application on publicly available <a href="https://data.cityofchicago.org/">Chicago</a> and <a href="https://www.yelp.com/developers">Yelp</a> data.
    <ul>
     <li>Performed data discovery, integration, and visualization on <a href="https://data.cityofchicago.org/">Chicago</a> datasets using Pandas, Numpy, and React Recharts.</li>
     <li>Achieved 81.9% sentiment analysis accuracy using Multiplicative LSTMs on Yelp Reviews dataset.</li>
     <li>Achieved 91.3% accuracy predicting types of robberies occuring in Chicago for the Summer of 2018 based on previous crime and weather datasets.</li>
   </ul>
 </td>
</tr>

<tr onmouseout="deepburst_stop()" onmouseover="deepburst_start()" >
  <td width="25%">
    <div class="one">
      <div class="two" id = 'deepburst_image'><img class="portrait_image" src='projects/lifeguard/after.png'></div>
      <img class="portrait_image" src='projects/lifeguard/before.png'>
    </div>
    <script type="text/javascript">
      function deepburst_start() {
        document.getElementById('deepburst_image').style.opacity = "1";
      }
      function deepburst_stop() {
        document.getElementById('deepburst_image').style.opacity = "0";
      }
      deepburst_stop()
    </script>
  </td>
  <td valign="top" width="75%">
    <a href="https://devpost.com/software/lifeguard-io">
      <papertitle>LifeGuard: Action Recognition of Drowning while Swimming</papertitle>
    </a>
    <br>
    <a href="https://devpost.com/sswarnakar">Sudipta Swarnakar</a>,
    <strong>Amlaan Bhoi</strong>,
    <a href="https://devpost.com/ChetanVelivela">Chetan Velivela</a>
    <br>
    <em>HackHarvard</em>, 2017<!--   &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>  --><br>
    <a href ="https://devpost.com/software/lifeguard-io">devpost</a>
    <br>
    We trained a 3D Convolutional Neural Network model on Microsoft Azure to detect drowning people in swimming pools. We also created the bounding boxes for our train, test, and validation set.
  </td>
</tr>

<tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
  <td width="25%">
    <div class="one">
      <div class="two" id = 'hdrnet_image'><img class="portrait_image" src='projects/ar/after.png'></div>
      <img class="portrait_image" src='projects/ar/before.png'>
    </div>
    <script type="text/javascript">
      function hdrnet_start() {
        document.getElementById('hdrnet_image').style.opacity = "1";
      }
      function hdrnet_stop() {
        document.getElementById('hdrnet_image').style.opacity = "0";
      }
      hdrnet_stop()
    </script>
  </td>
  <td valign="top" width="75%">
    <a href="https://github.com/dkaushik94/ARyouthereyet">
      <papertitle>ARYouThereYet</papertitle></a><br>
      <a href="https://sandeepjoshi1910.github.io/">Sandeep Joshi</a>, <strong>Amlaan Bhoi</strong>, <a href="https://github.com/dkaushik94">Debojit Kaushik</a> 
      <br>
      <em>Virtual and Augmented Reality</em>, Fall 2017 <br>
      <a href="https://debojitkaushikblog.wordpress.com/finally-some-augmented-reality-aryouthereyet/">project page</a>
      |
      <a href="https://github.com/dkaushik94/ARyouthereyet">code</a>
      |
      <a href="https://www.youtube.com/watch?v=U6ChLMEDQh0&t=5s">video</a>
      <br>
      An ARKit iOS application utilizing Google Maps and Mapbox APIs to show nearby attractions in Augmented Reality with support for visualizing the distance, detailed description of places, an AR walking guide to destinations, support for saving favorite places, and more.
    </td>
  </tr>

  <tr onmouseout="ffcc_stop()" onmouseover="ffcc_start()" >
    <td width="25%">
      <div class="one">
        <div class="two" id = 'ffcc_image'><img class="portrait_image" src='projects/autocolor/after.png'></div>
        <img class="portrait_image" src='projects/autocolor/before.png'>
      </div>
      <script type="text/javascript">
        function ffcc_start() {
          document.getElementById('ffcc_image').style.opacity = "1";
        }
        function ffcc_stop() {
          document.getElementById('ffcc_image').style.opacity = "0";
        }
        ffcc_stop()
      </script>
    </td>
    <td valign="top" width="75%">
      <a href="https://github.com/abhoi/AutoColor">
        <papertitle>AutoColor: Color Segmentation using Clustering</papertitle></a><br>
        <strong>Amlaan Bhoi</strong> <br>
        <em>Summer</em>, 2017 <br>
        <a href ="https://github.com/abhoi/AutoColor">code</a>
        <br>
        A K-means clustering algorithm using <a href="https://opencv.org/">OpenCV</a> and <a href="http://scikit-learn.org/stable/">Scikit-Learn</a> that detects <i>K</i> dominant colors in an image. Autopicks K using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html">Silhouette Coefficient</a> metric and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html">MiniBatchKMeans</a> for testing.
      </td>
    </tr>
  </table>
  <table style="width: 100%; max-width: 800px; border:0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
    <tr>
      <td>
        <br>
        <hr>
        <center>
         <quote><i>“There are many problems in vision where getting 50% of the solution takes one minute, getting to 90% can take you a day, getting to 99% may take you five years, and 99.99% may be not in your lifetime.”</i></quote> <foot> ~ <a href="https://youtu.be/LRYkH-fAVGE?t=390">Jitendra Malik</a></foot>
       <br>
       <foot>Design stolen from <u><a href="https://jonbarron.info/">here</a></u> & inspiration from <u><a href="http://ranjithakumar.net/">here</a></u></foot>
     </center>
     </td>
   </tr>
 </table>
</td>
</tr>
</table>
</body>
</html>

